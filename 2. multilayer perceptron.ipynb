{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron\n",
    "\n",
    "Ici nous allons utiliser tensorflow pour creer un reseau de perceptron avec une couche cachee, la majorite du code et des explications du debut sont identiques au premier tutoriel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le code qui suit nous allons charger les donnees MNIST, le code se chargera de telecharger les donnees pour vous, laissez lui un peu de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e9c74f96a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "s = ax.imshow(random.choice(mnist.test.images).reshape((28,28)), cmap = matplotlib.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessus un exemple aleatoire de chiffre MNIST. Le code a peu d'interet, ne vous attardez pas dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "classes = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos images font 28x28 pixels, nous avons donc besoin d'un moyen de donner ces images a notre modele, comme nous ne les envoyons pas au modele pendant que nous le construisons nous utilisons un **placeholder**.\n",
    "\n",
    "Nous utilisons egalement un **placeholder** pour les classes. Par exemple un chiffre '1' aura pour classe [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] et le chiffre '5' aura pour classe [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], c'est pourquoi nous avons 10 comme dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_weights(x, y):\n",
    "    return tf.Variable(tf.truncated_normal([x, y], stddev=1./x))\n",
    "\n",
    "def generate_bias(x):\n",
    "    return tf.Variable(tf.truncated_normal([x], stddev=1./x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deux fonctions permettant d'initialiser les poids et les bias aleatoirement, pour eviter d'avoir un gradient a 0 ce qui empecherait l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_hidden = generate_weights(28*28, 128)\n",
    "b_hidden = generate_bias(128)\n",
    "\n",
    "W = generate_weights(128, 10)\n",
    "b = generate_bias(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila le modele, nous utilisons 10 neurones, 1 par chiffre possible, chaque neurone prend en entree les sorties de 128 autres neurones qui prennent en entrees tous les pixels nous avons donc besoin d'une matrice 28x28 = 784, 784x128 et d'une matrice 128x10.\n",
    "\n",
    "La variable b est notre bias, nous reviendrons sur son utilite dans un prochain chapitre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hidden = tf.nn.relu(tf.matmul(x, W_hidden) + b_hidden)\n",
    "\n",
    "y = tf.matmul(y_hidden, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous multiplions x qui etait notre placeholder de dimension Nx784 avec notre matrice W_hidden de dimension 784x128 ce qui donne une matrice Nx128 a laquelle nous ajoutons notre bias. Nous appliquons ensuite une fonction non lineaire **relu** sur le resultat, vous pouvez essayer d'utiliser **softmax** afin de comparer les resultats. Nous utilisons le resultat avec notre matrice 128x10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=classes, logits=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous definissons notre fonction de cout, elle applique egalement la fonction softmax sur **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entrainement = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons la methode de descente de gradient pour minimiser notre fonction de cout. \n",
    "\n",
    "![descente de gradient](http://images.slideplayer.fr/4/1570133/slides/slide_97.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modele etant fini nous pouvons lancer une session tensorflow et initialiser toutes nos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(entrainement, feed_dict={x: batch_xs, classes: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de reussite: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "prediction = tf.equal(tf.argmax(y, 1), tf.argmax(classes, 1))\n",
    "performance = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "taux_reussite = sess.run(performance, feed_dict={x: mnist.test.images,\n",
    "                                  classes: mnist.test.labels})\n",
    "\n",
    "print(\"Taux de reussite: \" + str(taux_reussite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila qui conclu le chapitre sur les reseaux avec plusieurs couches!\n",
    "Vous pouvez voir que nous obtenons un resultat d'environ 96% compare au 91% du modele simple.\n",
    "\n",
    "**Exercice 1:** Utiliser la fonction prediction sur une image aleatoire random.choice(mnist.test.images) et l'afficher afin de verifier si vous etes d'accord avec la prediction du modele.\n",
    "\n",
    "**Exercice 2:** Changer le nombre de neurones dans la couche cache (128 ici) pour voir l'effet sur le taux de reussite.\n",
    "\n",
    "**Exercice 3:** Ajouter une couche supplementaire au reseau et voir l'effet sur le taux de reussite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
